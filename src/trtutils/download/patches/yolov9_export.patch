--- export.py	2025-10-18 20:14:21.258127553 -0500
+++ export_fix.py	2025-10-18 20:34:54.810977355 -0500
@@ -127,13 +127,10 @@
     # Simplify
     if simplify:
         try:
-            cuda = torch.cuda.is_available()
-            check_requirements(('onnxruntime-gpu' if cuda else 'onnxruntime', 'onnx-simplifier>=0.4.1'))
-            import onnxsim
-
-            LOGGER.info(f'{prefix} simplifying with onnx-simplifier {onnxsim.__version__}...')
-            model_onnx, check = onnxsim.simplify(model_onnx)
-            assert check, 'assert check failed'
+            import onnxslim
+
+            LOGGER.info(f'{prefix} simplifying with onnxslim {onnxslim.__version__}...')
+            model_onnx = onnxslim.slim(model_onnx)
             onnx.save(model_onnx, f)
         except Exception as e:
             LOGGER.info(f'{prefix} simplifier failure: {e}')
@@ -147,48 +144,75 @@
     import onnx
     LOGGER.info(f'\n{prefix} starting export with onnx {onnx.__version__}...')
     f = os.path.splitext(file)[0] + "-end2end.onnx"
-    batch_size = 'batch'
-
-    dynamic_axes = {'images': {0 : 'batch', 2: 'height', 3:'width'}, } # variable length axes
+    batch_size = 1
 
-    output_axes = {
-                    'num_dets': {0: 'batch'},
-                    'det_boxes': {0: 'batch'},
-                    'det_scores': {0: 'batch'},
-                    'det_classes': {0: 'batch'},
-                }
-    dynamic_axes.update(output_axes)
     model = End2End(model, topk_all, iou_thres, conf_thres, None ,device, labels)
 
     output_names = ['num_dets', 'det_boxes', 'det_scores', 'det_classes']
-    shapes = [ batch_size, 1,  batch_size,  topk_all, 4,
-               batch_size,  topk_all,  batch_size,  topk_all]
-
-    torch.onnx.export(model, 
-                          im, 
-                          f, 
-                          verbose=False, 
-                          export_params=True,       # store the trained parameter weights inside the model file
-                          opset_version=12, 
-                          do_constant_folding=True, # whether to execute constant folding for optimization
-                          input_names=['images'],
-                          output_names=output_names,
-                          dynamic_axes=dynamic_axes)
+    output_shapes = {
+        'num_dets': [batch_size],
+        'det_boxes': [batch_size, topk_all, 4],
+        'det_scores': [batch_size, topk_all],
+        'det_classes': [batch_size, topk_all],
+    }
+
+    torch.onnx.export(
+        model, 
+        im, 
+        f, 
+        verbose=False, 
+        export_params=True,       # store the trained parameter weights inside the model file
+        opset_version=12, 
+        do_constant_folding=True, # whether to execute constant folding for optimization
+        input_names=['images'],
+        output_names=output_names,
+    )
 
     # Checks
     model_onnx = onnx.load(f)  # load onnx model
     onnx.checker.check_model(model_onnx)  # check onnx model
-    for i in model_onnx.graph.output:
-        for j in i.type.tensor_type.shape.dim:
-            j.dim_param = str(shapes.pop(0))
+    output_dtype_map = {
+        'num_dets': onnx.TensorProto.INT32,
+        'det_boxes': onnx.TensorProto.FLOAT,
+        'det_scores': onnx.TensorProto.FLOAT,
+        'det_classes': onnx.TensorProto.INT32,
+    }
+    input_dtype_map = {
+        'images': onnx.TensorProto.FLOAT,
+    }
+    for graph_input in model_onnx.graph.input:
+        graph_input.type.tensor_type.elem_type = input_dtype_map.get(
+            graph_input.name,
+            onnx.TensorProto.FLOAT,
+        )
+        dims = list(im.shape)
+        for dim_proto, dim_value in zip(graph_input.type.tensor_type.shape.dim, dims):
+            dim_proto.dim_value = int(dim_value)
+    for output in model_onnx.graph.output:
+        dims = output_shapes.get(output.name)
+        if dims:
+            for dim_proto, dim_value in zip(output.type.tensor_type.shape.dim, dims):
+                dim_proto.dim_value = int(dim_value)
+        elem_type = output_dtype_map.get(output.name)
+        if elem_type is not None:
+            output.type.tensor_type.elem_type = elem_type
+    # Some exporters register outputs in value_info instead; ensure dtype/shape there too
+    for value_info in model_onnx.graph.value_info:
+        elem_type = output_dtype_map.get(value_info.name)
+        if elem_type is not None:
+            value_info.type.tensor_type.elem_type = elem_type
+            dims = output_shapes.get(value_info.name)
+            if dims:
+                for dim_proto, dim_value in zip(value_info.type.tensor_type.shape.dim, dims):
+                    dim_proto.dim_value = int(dim_value)
 
     if simplify:
         try:
-            import onnxsim
+            import onnxslim
 
-            print('\nStarting to simplify ONNX...')
-            model_onnx, check = onnxsim.simplify(model_onnx)
-            assert check, 'assert check failed'
+            LOGGER.info(f'{prefix} simplifying with onnxslim {onnxslim.__version__}...')
+            model_onnx = onnxslim.slim(model_onnx)
+            onnx.save(model_onnx, f)
         except Exception as e:
             print(f'Simplifier failure: {e}')
 
