--- /tmp/export_original.py	2026-02-04 18:41:56.919322221 -0600
+++ /tmp/export_patched.py	2026-02-04 18:48:08.156457329 -0600
@@ -127,18 +127,15 @@
     # Simplify
     if simplify:
         try:
-            cuda = torch.cuda.is_available()
-            check_requirements(('onnxruntime-gpu' if cuda else 'onnxruntime', 'onnx-simplifier>=0.4.1'))
-            import onnxsim
-
-            LOGGER.info(f'{prefix} simplifying with onnx-simplifier {onnxsim.__version__}...')
-            model_onnx, check = onnxsim.simplify(model_onnx)
-            assert check, 'assert check failed'
+            import onnxslim
+
+            LOGGER.info(f'{prefix} simplifying with onnxslim {onnxslim.__version__}...')
+            model_onnx = onnxslim.slim(model_onnx)
             onnx.save(model_onnx, f)
         except Exception as e:
             LOGGER.info(f'{prefix} simplifier failure: {e}')
     return f, model_onnx
-    
+
 
 @try_export
 def export_onnx_end2end(model, im, file, simplify, topk_all, iou_thres, conf_thres, device, labels, prefix=colorstr('ONNX END2END:')):
@@ -147,54 +144,86 @@
     import onnx
     LOGGER.info(f'\n{prefix} starting export with onnx {onnx.__version__}...')
     f = os.path.splitext(file)[0] + "-end2end.onnx"
-    batch_size = 'batch'
+    batch_size = 1
 
-    dynamic_axes = {'images': {0 : 'batch', 2: 'height', 3:'width'}, } # variable length axes
-
-    output_axes = {
-                    'num_dets': {0: 'batch'},
-                    'det_boxes': {0: 'batch'},
-                    'det_scores': {0: 'batch'},
-                    'det_classes': {0: 'batch'},
-                }
-    dynamic_axes.update(output_axes)
     model = End2End(model, topk_all, iou_thres, conf_thres, None ,device, labels)
 
     output_names = ['num_dets', 'det_boxes', 'det_scores', 'det_classes']
-    shapes = [ batch_size, 1,  batch_size,  topk_all, 4,
-               batch_size,  topk_all,  batch_size,  topk_all]
-
-    torch.onnx.export(model, 
-                          im, 
-                          f, 
-                          verbose=False, 
-                          export_params=True,       # store the trained parameter weights inside the model file
-                          opset_version=12, 
-                          do_constant_folding=True, # whether to execute constant folding for optimization
-                          input_names=['images'],
-                          output_names=output_names,
-                          dynamic_axes=dynamic_axes)
+    output_shapes = {
+        'num_dets': [batch_size],
+        'det_boxes': [batch_size, topk_all, 4],
+        'det_scores': [batch_size, topk_all],
+        'det_classes': [batch_size, topk_all],
+    }
+    output_dtype_map = {
+        'num_dets': onnx.TensorProto.INT32,
+        'det_boxes': onnx.TensorProto.FLOAT,
+        'det_scores': onnx.TensorProto.FLOAT,
+        'det_classes': onnx.TensorProto.INT32,
+    }
+
+    torch.onnx.export(
+        model,
+        im,
+        f,
+        verbose=False,
+        export_params=True,       # store the trained parameter weights inside the model file
+        opset_version=12,
+        do_constant_folding=True, # whether to execute constant folding for optimization
+        input_names=['images'],
+        output_names=output_names,
+    )
 
     # Checks
     model_onnx = onnx.load(f)  # load onnx model
     onnx.checker.check_model(model_onnx)  # check onnx model
-    for i in model_onnx.graph.output:
-        for j in i.type.tensor_type.shape.dim:
-            j.dim_param = str(shapes.pop(0))
 
     if simplify:
         try:
-            import onnxsim
+            import onnxslim
 
-            print('\nStarting to simplify ONNX...')
-            model_onnx, check = onnxsim.simplify(model_onnx)
-            assert check, 'assert check failed'
+            LOGGER.info(f'{prefix} simplifying with onnxslim {onnxslim.__version__}...')
+            model_onnx = onnxslim.slim(model_onnx)
         except Exception as e:
             print(f'Simplifier failure: {e}')
 
-        # print(onnx.helper.printable_graph(onnx_model.graph))  # print a human readable model
-        onnx.save(model_onnx,f)
-        print('ONNX export success, saved as %s' % f)
+    # Set input dtypes and shapes (must be done after simplify which may strip them)
+    for graph_input in model_onnx.graph.input:
+        graph_input.type.tensor_type.elem_type = onnx.TensorProto.FLOAT
+        dims = list(im.shape)
+        for dim_proto, dim_value in zip(graph_input.type.tensor_type.shape.dim, dims):
+            dim_proto.dim_value = int(dim_value)
+
+    # Set output dtypes and shapes (must be done after simplify which may strip them)
+    for output in model_onnx.graph.output:
+        dims = output_shapes.get(output.name)
+        if dims:
+            # Clear existing dims and add new ones
+            while len(output.type.tensor_type.shape.dim) > 0:
+                output.type.tensor_type.shape.dim.pop()
+            for dim_value in dims:
+                dim_proto = output.type.tensor_type.shape.dim.add()
+                dim_proto.dim_value = int(dim_value)
+        elem_type = output_dtype_map.get(output.name)
+        if elem_type is not None:
+            output.type.tensor_type.elem_type = elem_type
+
+    # Some exporters register outputs in value_info instead; ensure dtype/shape there too
+    for value_info in model_onnx.graph.value_info:
+        elem_type = output_dtype_map.get(value_info.name)
+        if elem_type is not None:
+            value_info.type.tensor_type.elem_type = elem_type
+            dims = output_shapes.get(value_info.name)
+            if dims:
+                while len(value_info.type.tensor_type.shape.dim) > 0:
+                    value_info.type.tensor_type.shape.dim.pop()
+                for dim_value in dims:
+                    dim_proto = value_info.type.tensor_type.shape.dim.add()
+                    dim_proto.dim_value = int(dim_value)
+
+    # Save the final model with correct dtypes
+    onnx.save(model_onnx, f)
+    LOGGER.info(f'{prefix} export success, saved as {f}')
     return f, model_onnx
 
 
@@ -666,7 +695,7 @@
         help='torchscript, onnx, onnx_end2end, openvino, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle')
     opt = parser.parse_args()
 
-    if 'onnx_end2end' in opt.include:  
+    if 'onnx_end2end' in opt.include:
         opt.simplify = True
         opt.dynamic = True
         opt.inplace = True
